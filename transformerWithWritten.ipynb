{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Transformer from Scratch (Part 1 - Basic Building Blocks)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is an example problem set that will guide you through building a **transformer model from scratch**, based on the seminal paper *\"Attention is All You Need\"* (Vaswani et al., 2017). We'll be implementing a transformer for **English-to-French machine translation**, following the architecture described in the original paper.\n",
    "\n",
    "While future problem sets may vary, they will generally follow this **pedagogical style**: a **bottom-up approach** where we first create and understand the fundamental building blocks, then gradually abstract them away to build more complex systems.\n",
    "\n",
    "In this notebook, we‚Äôll start by implementing **basic components** like:\n",
    "- **Self-Attention Mechanisms** ‚Äì the core idea behind transformers,\n",
    "- **Feed-Forward Networks** ‚Äì how the model processes data,\n",
    "- **Positional Encoding** ‚Äì how the model understands word order.\n",
    "\n",
    "Then, we‚Äôll **combine these into transformer blocks** and finally **assemble a complete transformer model** for translation. This step-by-step approach will help you develop a deep understanding of **how these models actually work under the hood** rather than treating them as black boxes.\n",
    "\n",
    "---\n",
    "\n",
    "## Our Learning Philosophy \n",
    "\n",
    "We firmly believe that the best way to master complex concepts‚Äîespecially in deep learning‚Äîis through a **hands-on, bottom-up approach**. Instead of simply using pre-built libraries, this problem set challenges you to **construct the model from first principles**. By working through each component step by step, you'll gain a much deeper intuition about how transformers function.\n",
    "\n",
    "This philosophy aligns with the idea of **active learning**:  \n",
    "‚úîÔ∏è Engaging directly with the material through **coding, experimentation, and problem-solving**.  \n",
    "‚úîÔ∏è Thinking critically, debugging, and refining your understanding **through trial and error**.  \n",
    "‚úîÔ∏è **Searching for solutions online, revisiting lecture notes, and consulting research papers**‚Äîjust like real machine learning engineers and researchers do!\n",
    "\n",
    "This problem set also **mirrors how research and industry work**. Engineers don‚Äôt just memorize formulas‚Äîthey break down complex systems, experiment with different approaches, and continuously iterate on their solutions.\n",
    "\n",
    "---\n",
    "\n",
    "## What to Expect\n",
    "\n",
    "As we progress, our goal is to **gradually abstract away** the lower-level components, allowing you to focus on **high-level architectural design, optimization, and real-world deployment**. By the end of this series, you won‚Äôt just understand transformers theoretically‚Äîyou‚Äôll have **practical experience** in modifying, optimizing, and applying them to diverse applications beyond machine translation. \n",
    "\n",
    "You also don‚Äôt need to grasp every nitty-gritty detail (and we definitely don't expect you too!) ‚Äî**as long as you understand the main idea, that‚Äôs good enough!** This **especially** applies to the code ‚Äî we know it's very complicated, and we didn't expect you to be a professional Python programmer before taking this class. The fill in the blank questions were made to be relatively simple - it should require whatever thinking we believe is necessary! However, we do recommend reading through the code, roughly understanding what each line does through the comments (without knowing exactly the operations).\n",
    "\n",
    "**Anyways, be sure to look out for #TODOs throughout the notebook!** Please, for the fill in the blank ones, do not modify any other part of the code!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"transformer_architecture.png\" alt=\"Transformer architecture diagram from 'Attention is All You Need' paper\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this may look complicated at first, let's simplify it, and build it step by step! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Understanding Word Embeddings in Transformers\n",
    "\n",
    "## **What are Embeddings?**\n",
    "Before a transformer can process words, it needs to **convert them into numbers**. Computers don‚Äôt understand text, so we represent words as **vectors** (lists of numbers). These numbers are called **word embedding**.\n",
    "\n",
    "Instead of using simple numbers (like \"1\" for \"apple\" and \"2\" for \"orange\"), we use **high-dimensional vectors** that store information about each word‚Äôs meaning. This helps the model understand relationships between words.\n",
    "\n",
    "For example:\n",
    "- The words **\"king\"** and **\"queen\"** might have similar embeddings because they are related.\n",
    "- The word **\"dog\"** would have an embedding closer to **\"puppy\"** than to **\"banana\"**.\n",
    "\n",
    "## **What Does This Code Do?**\n",
    "We define a class called `InputEmbeddings`, which converts word indices into **embedding vectors** and **scales them** properly for the transformer model.\n",
    "\n",
    "üí° **Fill in the blank!** Try to complete the missing part in the code below to understand how word embeddings work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    This class converts word indices into dense vector embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the embedding layer.\n",
    "\n",
    "        Parameters:\n",
    "        - d_model (int): The size of each embedding vector (the number of features per word).\n",
    "          Example: If d_model = 512, each word is represented as a 512-dimensional vector.\n",
    "        - vocab_size (int): The number of unique words in our vocabulary.\n",
    "          Example: If vocab_size = 10,000, our vocabulary contains 10,000 different words.\n",
    "        \"\"\"\n",
    "        super().__init__()  # TODO: Written Task 1.1\",\n",
    "                            # This super() function call is a property of something called Inheritance.\",\n",
    "                            # First read about it here: https://www.geeksforgeeks.org/python-super-with-__init__-method/\",\n",
    "                            # Then, write a short paragraph explaining your understanding of this call.\",\n",
    "        # TODO: Fill in the blank\n",
    "        self.d_model = ____  # Store the embedding size\n",
    "        self.vocab_size = ____  # Store the vocabulary size\n",
    "\n",
    "        # Create an embedding layer that maps each word index to a vector of size d_model\n",
    "        # Example: If vocab_size = 10,000 and d_model = 512, \n",
    "        #          nn.Embedding(10000, 512) creates a lookup table of 10,000 rows and 512 columns.\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Converts a batch of word indices into their corresponding embeddings.\n",
    "        \n",
    "        Parameters:\n",
    "        - x: A tensor of shape (batch_size, seq_len) where each value is a word index.\n",
    "          Example: Suppose batch_size = 2, seq_len = 4\n",
    "          Input x could be:\n",
    "          ```\n",
    "          x = torch.tensor([\n",
    "              [12, 305, 76, 4821],  # First sequence (each number is a word index)\n",
    "              [519, 3, 78, 9023]    # Second sequence (each number is a word index)\n",
    "          ])\n",
    "          ```\n",
    "        \n",
    "        Returns:\n",
    "        - A tensor of shape (batch_size, seq_len, d_model) containing the embeddings.\n",
    "          Example: If batch_size = 2, seq_len = 4, d_model = 512\n",
    "          Output will have shape (2, 4, 512).\n",
    "        \"\"\"\n",
    "        # TODO Written Task 1.2: \\n\",\n",
    "        # Explain the role of batch_size and seq_len. Why might we use batch_size 2 and seq_len 4 instead of an array with length 8?\"\n",
    "    \n",
    "        # Convert word indices into embedding vectors using the embedding layer.\n",
    "        # This retrieves the 512-dimensional vector corresponding to each word index.\n",
    "        # Example: Suppose word index 12 maps to:\n",
    "        # embeddings[12] = [0.1, -0.2, 0.5, ..., 0.3] (512 values)\n",
    "\n",
    "        # TODO: Fill in the blank\n",
    "        embeddings = self.____(x)  # Shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Scale the embeddings by the square root of d_model (as suggested in the Transformer paper)\n",
    "        # Why? This helps stabilize training by ensuring embeddings have a reasonable scale.\n",
    "        # Example: If d_model = 512, then sqrt(512) ‚âà 22.63\n",
    "\n",
    "        # TODO: Fill in the blank\n",
    "        scaled_embeddings = embeddings * math.___(self.d_model)\n",
    "\n",
    "        # Example: If embeddings[12] was [0.1, -0.2, ..., 0.3]\n",
    "        # Then scaled_embeddings[12] = [0.1 * 22.63, -0.2 * 22.63, ..., 0.3 * 22.63]\n",
    "        # This ensures that the magnitude of embedding vectors is appropriately scaled.\n",
    "\n",
    "        # TODO: Fill in the blank\n",
    "        return ____  # Shape: (batch_size, seq_len, d_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Positional Encoding in Transformers\n",
    "\n",
    "## **Why Do We Need Positional Encoding?**\n",
    "Transformers **do not have built-in order awareness**. Unlike RNNs, which process words sequentially, transformers **process all words at once**. This is powerful but creates a problem:  \n",
    "*How does the model know the order of words in a sentence?*\n",
    "\n",
    "**Solution: Positional Encoding!**  \n",
    "Instead of learning word order like an RNN, the transformer **adds a mathematical pattern** to word embeddings. This pattern helps the model **understand word positions**.\n",
    "\n",
    "### **How Does It Work?**\n",
    "Each position in a sentence gets a unique vector, created using **sine** and **cosine** functions. If you don't know what this means, it's okay. The main idea is that each position has an unique identifier.  \n",
    "- Words at different positions have **different encoding values**.\n",
    "- Words at similar positions have **similar encoding patterns**.\n",
    "\n",
    "**Example:**  \n",
    "If \"I love pizza\" and \"Pizza love I\" have the same word embeddings, the positional encoding ensures they are processed **differently**.\n",
    "\n",
    "---\n",
    "\n",
    "## **What Does This Code Do?**\n",
    "The `PositionalEncoding` class:\n",
    "1. **Creates a matrix (`pe`)** that stores position information for every word.\n",
    "2. Uses **sine (sin) for even indices** and **cosine (cos) for odd indices** to generate a pattern.\n",
    "3. **Adds this pattern** to word embeddings before passing them through the transformer.\n",
    "\n",
    "üí° **Fill in the blank!** Try to complete the missing part in the code below to understand how positional encoding works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO: Written Task 1.3 \n",
    " # Now that you know more about what inheritance, let's see what attributes our classes are inheriting. \n",
    " # We are inheriting from the class nn.module. Read about it here: https://pytorch.org/docs/stable/generated/torch.nn.Module.html \n",
    " # Explain what the methods (functions) init(), forward(), train(), and params() do.\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    This class creates positional encodings and adds them to word embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the positional encoding.\n",
    "\n",
    "        Parameters:\n",
    "        - d_model (int): The size of each embedding vector.\n",
    "        - seq_len (int): The maximum length of a sequence (number of words).\n",
    "        - dropout (float): Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # Store embedding size\n",
    "        self.seq_len = seq_len  # Store maximum sequence length\n",
    "        # Don't worry about dropout or regularization for now.\n",
    "        self.dropout = nn.Dropout(dropout)  # Apply dropout for regularization\n",
    "\n",
    "        # Create a matrix to store positional encodings (shape: seq_len x d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "        # Create a vector of positions from 0 to seq_len-1 (shape: seq_len x 1)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Compute a scaling factor for sine and cosine functions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply sine function to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "        # Apply cosine function to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add a batch dimension (1, seq_len, d_model) since all batches share the same encoding\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Register positional encoding as a buffer (not a learnable parameter)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Adds positional encodings to the input embeddings.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "        - Tensor with positional encodings added, same shape as input.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Fill in the blank. Add positional encoding to the input embeddings\n",
    "        x = ___ + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Layer Normalization in Transformers\n",
    "\n",
    "## **Why Do We Need Layer Normalization?**\n",
    "When training deep neural networks, some neurons might produce **very large** or **very small** values, making learning unstable. To **fix this**, we use **normalization techniques** that keep the model's outputs at a **consistent scale**.\n",
    "\n",
    "### **What is Normalization?**\n",
    "Normalization **adjusts values** so they have:\n",
    "\n",
    "‚úîÔ∏è A **mean (average) of 0**  \n",
    "‚úîÔ∏è A **standard deviation of 1**  \n",
    "\n",
    "This makes training **faster** and **more stable** by ensuring no neuron dominates the learning process.\n",
    "\n",
    "### Normalization Formula\n",
    "\n",
    "The formula for standardization is $X' = \\frac{X - \\mu}{\\sigma}$.\n",
    "\n",
    "Where:\n",
    "- \\(X'\\) is the standardized value,\n",
    "- \\(X\\) is the original value,\n",
    "- $\\mu$ is the mean of the dataset,\n",
    "- $\\sigma$ is the standard deviation of the dataset.\n",
    "\n",
    "This method transforms the data to have a mean of 0 and a standard deviation of 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    \"\"\"\n",
    "    This class applies Layer Normalization to stabilize transformer training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features: int, eps: float = 10**-6) -> None:\n",
    "        \"\"\"\n",
    "        Initializes layer normalization.\n",
    "\n",
    "        Parameters:\n",
    "        - features (int): The number of features (same as embedding size).\n",
    "        - eps (float): A small number to prevent division by zero.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps  # Small value to prevent numerical issues\n",
    "\n",
    "        # Learnable scaling parameter (alpha), initialized as ones\n",
    "        self.alpha = nn.Parameter(torch.ones(features))  \n",
    "\n",
    "        # Learnable bias parameter, initialized as zeros\n",
    "        self.bias = nn.Parameter(torch.zeros(features))  \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Applies layer normalization to the input.\n",
    "\n",
    "        Parameters:\n",
    "        - x: A tensor of shape (batch, seq_len, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "        - Normalized tensor with the same shape as input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the mean across the last dimension (hidden size)\n",
    "        mean = x.mean(dim=-1, keepdim=True)  # Shape: (batch, seq_len, 1)\n",
    "\n",
    "        # Compute the standard deviation across the last dimension\n",
    "        std = x.std(dim=-1, keepdim=True)  # Shape: (batch, seq_len, 1)\n",
    "\n",
    "        # TODO: Fill in the blank.Normalize x by subtracting mean and dividing by standard deviation\n",
    "        x_normalized = (__________) / (std + self.eps)  # Fill in the missing part!\n",
    "\n",
    "        # Apply learnable scaling (alpha) and bias\n",
    "        return self.alpha * x_normalized + self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Feed-Forward Networks in Transformers\n",
    "\n",
    "## **Why Do We Need a Feed-Forward Network?**\n",
    "After processing words through **self-attention**, the transformer needs a way to **further analyze** the information before passing it to the next layer.  \n",
    "\n",
    "**Solution:** Each transformer block contains a **Feed-Forward Network (FFN)** that helps the model **refine** its understanding of the words.\n",
    "\n",
    "---\n",
    "\n",
    "## **What Does the Feed-Forward Network Do?**\n",
    "The FFN is like a **mini neural network** inside the transformer. It has two steps:\n",
    "1. **Expands information** ‚Äì It increases the number of features (hidden units) so the model can process more complex patterns.\n",
    "2. **Compresses information** ‚Äì It reduces the features back to the original size, keeping only the most useful details.\n",
    "\n",
    "### **Steps in Simple Terms:**\n",
    "1. Convert each word‚Äôs information into a **bigger** representation using a **linear layer**.\n",
    "2. Apply **ReLU activation**, which helps the model keep only **important information**.\n",
    "3. Apply **dropout** to randomly turn off some neurons (helps avoid overfitting).\n",
    "4. Convert it back to the original size using another **linear layer**.\n",
    "\n",
    "---\n",
    "\n",
    "## **What Does This Code Do?**\n",
    "The `FeedForwardBlock` class:\n",
    "1. **Expands** word features from `d_model` ‚Üí `d_ff` (more neurons).\n",
    "2. **Applies ReLU** to remove negative values (helps with learning).\n",
    "3. **Uses dropout** to prevent overfitting.\n",
    "4. **Reduces features** back from `d_ff` ‚Üí `d_model`.\n",
    "\n",
    "üí° **Fill in the blank!** Try to complete the missing part in the code below to understand how feed-forward networks work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines a Feed-Forward Network (FFN) used in transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the Feed-Forward block.\n",
    "\n",
    "        Parameters:\n",
    "        - d_model (int): The size of the word embeddings.\n",
    "        - d_ff (int): The expanded size (more neurons for better learning).\n",
    "        - dropout (float): The dropout rate to prevent overfitting.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # First linear layer expands the input size\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)  # Expands features (d_model ‚Üí d_ff)\n",
    "\n",
    "        # Dropout layer helps prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "\n",
    "        # Second linear layer reduces back to original size\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)  # Compresses features (d_ff ‚Üí d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Passes the input through the feed-forward network.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Tensor of shape (batch, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "        - Transformed tensor of the same shape as input.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Fill in the blank. Apply first linear layer, ReLU activation, dropout, then second linear layer\n",
    "        x_transformed = self.linear_2(self.dropout(torch.relu(_________)))  # Fill in the blank!\n",
    "\n",
    "        return x_transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Multi-Head Attention in Transformers\n",
    "\n",
    "## **What is Attention?**\n",
    "Attention is a way for a model to focus on **important words** in a sentence while processing it. Imagine reading a book‚Äîyou don't pay equal attention to every word. Instead, your brain focuses on key parts depending on the **context**.\n",
    "\n",
    "For example:  \n",
    "- In the sentence **\"She went to the store because she needed milk.\"**,  \n",
    "  - \"She\" at the end refers to \"She\" at the beginning.  \n",
    "  - The word **\"store\"** is important for understanding where \"she\" went.  \n",
    "\n",
    "**Attention allows the transformer to focus on important words!** Instead of looking at words **one by one**, the transformer looks at all words **at the same time** and decides which words are important/relate to each other.\n",
    "\n",
    "---\n",
    "\n",
    "## **What is Multi-Head Attention?**\n",
    "Instead of applying attention **once**, the transformer applies it **multiple times in parallel** using **multiple attention heads**. Each head focuses on **different aspects** of the sentence.\n",
    "\n",
    "**Example:**  \n",
    "- One attention head might focus on **subject-verb relationships**.  \n",
    "- Another might focus on **object references**.  \n",
    "- Another might focus on **word order**.\n",
    "\n",
    "### **Steps of Multi-Head Attention:**\n",
    "1. **Create three different versions of the input:**  \n",
    "   - **Query (`Q`)** ‚Üí What are we looking for?  \n",
    "   - **Key (`K`)** ‚Üí What information is available?  \n",
    "   - **Value (`V`)** ‚Üí What information should we return?\n",
    "\n",
    "2. **Compute attention scores** using the formula:  \n",
    "   $$\n",
    "   \\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "   $$\n",
    "   - The **softmax function** helps the model assign importance to different words. This formula looks complicated, but don't worry ‚Äî you don't need to understand it fully!\n",
    "\n",
    "3. **Apply attention separately for each head.**  \n",
    "4. **Merge all the heads together** to get the final output.\n",
    "\n",
    "---\n",
    "\n",
    "## **What is Masking?**\n",
    "Masking is used to **hide certain words** so the model doesn't \"cheat.\"\n",
    "\n",
    "**Padding Mask:** Prevents the model from paying attention to empty spaces in short sentences.  \n",
    "**Look-Ahead Mask:** Ensures the model only looks at **previous words** when predicting the next word (important for tasks like future word prediction).\n",
    "\n",
    "---\n",
    "\n",
    "## **What Does This Code Do?**\n",
    "The `MultiHeadAttentionBlock` class:\n",
    "1. **Creates multiple attention heads** to focus on different parts of the sentence.  \n",
    "2. **Computes attention scores** to decide which words are important.  \n",
    "3. **Uses masking** to prevent the model from looking at unwanted words.  \n",
    "4. **Combines all attention heads** and returns the final output.\n",
    "\n",
    "üí° **Fill in the blank!** Try to complete the missing part in the code below to understand how multi-head attention works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines Multi-Head Attention, which helps the transformer focus on important words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the Multi-Head Attention block.\n",
    "\n",
    "        Parameters:\n",
    "        - d_model (int): The size of the word embeddings.\n",
    "        - h (int): The number of attention heads.\n",
    "        - dropout (float): The dropout rate to prevent overfitting.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model  # Size of word embeddings\n",
    "        self.h = h  # Number of attention heads\n",
    "\n",
    "        # Ensure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model must be divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h  # Size of each attention head\n",
    "\n",
    "        # Create weight matrices for Query (Q), Key (K), Value (V), and Output (O)\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)  # Query weights\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)  # Key weights\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)  # Value weights\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)  # Output weights\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        \"\"\"\n",
    "        Computes scaled dot-product attention.\n",
    "\n",
    "        Parameters:\n",
    "        - query: Q (batch, h, seq_len, d_k)\n",
    "        - key: K (batch, h, seq_len, d_k)\n",
    "        - value: V (batch, h, seq_len, d_k)\n",
    "        - mask: Masking tensor (optional)\n",
    "        - dropout: Dropout layer\n",
    "\n",
    "        Returns:\n",
    "        - Attention output and attention scores\n",
    "        \"\"\"\n",
    "        d_k = query.shape[-1]  # Size of each attention head\n",
    "\n",
    "        # Compute attention scores using scaled dot product formula\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        # Apply masking (if provided)\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "\n",
    "        # Apply dropout (if provided)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        # Multiply scores with value matrix\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        \"\"\"\n",
    "        Computes multi-head attention.\n",
    "\n",
    "        Parameters:\n",
    "        - q: Query tensor (batch, seq_len, d_model)\n",
    "        - k: Key tensor (batch, seq_len, d_model)\n",
    "        - v: Value tensor (batch, seq_len, d_model)\n",
    "        - mask: Masking tensor (optional)\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor with multi-head attention applied.\n",
    "        \"\"\"\n",
    "\n",
    "        #TODO Written Task 1.4: Explain how the queries, keys, and values of each word interact with one another.\n",
    "        #Use this page to guide you if you need help: https://epichka.com/blog/2023/qkv-transformer/\n",
    "        \n",
    "        # TODO Coding: Fill in the blank. Apply weight matrices to get Q, K, V\n",
    "        query = self.w_q(q)\n",
    "        key = ______\n",
    "        value = ______\n",
    "\n",
    "        # Reshape Q, K, V for multi-head attention\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Compute attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Reshape the output tensor back to (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # TODO: Fill in the blank. Apply output weight matrix\n",
    "        return ____\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
