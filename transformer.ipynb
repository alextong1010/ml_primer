{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to ML Primer!\n",
    "\n",
    "# üöÄ Building a Transformer from Scratch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is an example problem set that will guide you through building a **transformer model from scratch**, based on the seminal paper *\"Attention is All You Need\"* (Vaswani et al., 2017). We'll be implementing a transformer for **English-to-French machine translation**, following the architecture described in the original paper.\n",
    "\n",
    "While future problem sets may vary, they will generally follow this **pedagogical style**: a **bottom-up approach** where we first create and understand the fundamental building blocks, then gradually abstract them away to build more complex systems.\n",
    "\n",
    "In this notebook, we‚Äôll start by implementing **basic components** like:\n",
    "- **Self-Attention Mechanisms** ‚Äì the core idea behind transformers,\n",
    "- **Feed-Forward Networks** ‚Äì how the model processes data,\n",
    "- **Positional Encoding** ‚Äì how the model understands word order.\n",
    "\n",
    "Then, we‚Äôll **combine these into transformer blocks** and finally **assemble a complete transformer model** for translation. This step-by-step approach will help you develop a deep understanding of **how these models actually work under the hood** rather than treating them as black boxes.\n",
    "\n",
    "---\n",
    "\n",
    "## Our Learning Philosophy üß†üí°\n",
    "\n",
    "We firmly believe that the best way to master complex concepts‚Äîespecially in deep learning‚Äîis through a **hands-on, bottom-up approach**. Instead of simply using pre-built transformer libraries, this problem set challenges you to **construct the model from first principles**. By working through each component step by step, you'll gain a much deeper intuition about how transformers function.\n",
    "\n",
    "This philosophy aligns with the idea of **active learning**:  \n",
    "‚úîÔ∏è Engaging directly with the material through **coding, experimentation, and problem-solving**.  \n",
    "‚úîÔ∏è Thinking critically, debugging, and refining your understanding **through trial and error**.  \n",
    "‚úîÔ∏è **Searching for solutions online, revisiting lecture notes, and consulting research papers**‚Äîjust like real machine learning engineers and researchers do!\n",
    "\n",
    "This problem set also **mirrors how research and industry work**. Engineers don‚Äôt just memorize formulas‚Äîthey break down complex systems, experiment with different approaches, and continuously iterate on their solutions. By adopting this mindset, you‚Äôll **not only develop technical expertise but also cultivate problem-solving skills** that are essential for innovation in AI and machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "## What to Expect üîéüìñ\n",
    "\n",
    "As we progress, our goal is to **gradually abstract away** the lower-level components, allowing you to focus on **high-level architectural design, optimization, and real-world deployment**. By the end of this series, you won‚Äôt just understand transformers theoretically‚Äîyou‚Äôll have **practical experience** in modifying, optimizing, and applying them to diverse applications beyond machine translation. \n",
    "\n",
    "So, **embrace the challenge, experiment fearlessly, and remember**:  \n",
    "üí° **Struggling with concepts at first is not a sign of failure‚Äîit's an essential part of deep learning** (both for neural networks and for humans!).  \n",
    "\n",
    "You don‚Äôt need to grasp every nitty-gritty detail (and we definitely don't expect you too!) ‚Äî**as long as you understand the main idea, that‚Äôs good enough!**\n",
    "\n",
    "**To do this, look for #TODOs throughout the notebook.**\n",
    "\n",
    "Let‚Äôs dive in! üöÄ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"transformer_architecture.png\" alt=\"Transformer architecture diagram from 'Attention is All You Need' paper\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this may look complicated at first, let's simplify it, and build it step by step! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# üî° Word Embeddings in Transformers\n",
    "\n",
    "## **What are Embeddings?**\n",
    "Before a transformer can process words, it needs to **convert them into numbers**. Computers don‚Äôt understand text, so we represent words as **vectors** (lists of numbers). These numbers are called **word embedding**.\n",
    "\n",
    "Instead of using simple numbers (like \"1\" for \"apple\" and \"2\" for \"orange\"), we use **high-dimensional vectors** that store information about each word‚Äôs meaning. This helps the model understand relationships between words.\n",
    "\n",
    "For example:\n",
    "- The words **\"king\"** and **\"queen\"** might have similar embeddings because they are related.\n",
    "- The word **\"dog\"** would have an embedding closer to **\"puppy\"** than to **\"banana\"**.\n",
    "\n",
    "## **What Does This Code Do?**\n",
    "We define a class called `InputEmbeddings`, which converts word indices into **embedding vectors** and **scales them** properly for the transformer model.\n",
    "\n",
    "üí° **Fill in the blank!** Try to complete the missing part in the code below to understand how word embeddings work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    This class converts word indices into dense vector embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the embedding layer.\n",
    "\n",
    "        Parameters:\n",
    "        - d_model (int): The size of each embedding vector (the number of features per word).\n",
    "          Example: If d_model = 512, each word is represented as a 512-dimensional vector.\n",
    "        - vocab_size (int): The number of unique words in our vocabulary.\n",
    "          Example: If vocab_size = 10,000, our vocabulary contains 10,000 different words.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Fill in the blank\n",
    "        self.d_model = ____  # Store the embedding size\n",
    "        self.vocab_size = ____  # Store the vocabulary size\n",
    "\n",
    "        # Create an embedding layer that maps each word index to a vector of size d_model\n",
    "        # Example: If vocab_size = 10,000 and d_model = 512, \n",
    "        #          nn.Embedding(10000, 512) creates a lookup table of 10,000 rows and 512 columns.\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Converts a batch of word indices into their corresponding embeddings.\n",
    "        \n",
    "        Parameters:\n",
    "        - x: A tensor of shape (batch_size, seq_len) where each value is a word index.\n",
    "          Example: Suppose batch_size = 2, seq_len = 4\n",
    "          Input x could be:\n",
    "          ```\n",
    "          x = torch.tensor([\n",
    "              [12, 305, 76, 4821],  # First sequence (each number is a word index)\n",
    "              [519, 3, 78, 9023]    # Second sequence (each number is a word index)\n",
    "          ])\n",
    "          ```\n",
    "        \n",
    "        Returns:\n",
    "        - A tensor of shape (batch_size, seq_len, d_model) containing the embeddings.\n",
    "          Example: If batch_size = 2, seq_len = 4, d_model = 512\n",
    "          Output will have shape (2, 4, 512).\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert word indices into embedding vectors using the embedding layer.\n",
    "        # This retrieves the 512-dimensional vector corresponding to each word index.\n",
    "        # Example: Suppose word index 12 maps to:\n",
    "        # embeddings[12] = [0.1, -0.2, 0.5, ..., 0.3] (512 values)\n",
    "\n",
    "        # TODO: Fill in the blank\n",
    "        embeddings = self.____(x)  # Shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Scale the embeddings by the square root of d_model (as suggested in the Transformer paper)\n",
    "        # Why? This helps stabilize training by ensuring embeddings have a reasonable scale.\n",
    "        # Example: If d_model = 512, then sqrt(512) ‚âà 22.63\n",
    "\n",
    "        # TODO: Fill in the blank\n",
    "        scaled_embeddings = embeddings * math.___(self.d_model)\n",
    "\n",
    "        # Example: If embeddings[12] was [0.1, -0.2, ..., 0.3]\n",
    "        # Then scaled_embeddings[12] = [0.1 * 22.63, -0.2 * 22.63, ..., 0.3 * 22.63]\n",
    "        # This ensures that the magnitude of embedding vectors is appropriately scaled.\n",
    "\n",
    "        # TODO: Fill in the blank\n",
    "        return ____  # Shape: (batch_size, seq_len, d_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
